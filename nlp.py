# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a62QF2nMo6QKlAiij8TIHlV0B8lh2829
"""

import os
import sys
import numpy as np
import random
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten, GlobalAveragePooling1D, Dense, Input, GlobalMaxPooling1D, Concatenate, Dropout, SimpleRNN
from keras.models import Sequential, Model
from keras.optimizers import RMSprop, Adam
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
import pandas as pd
import re
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import LearningRateScheduler
import matplotlib.pyplot as plt
from keras.layers import Reshape
from tensorflow.keras.callbacks import LearningRateScheduler
import matplotlib.pyplot as plt
import csv

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/glove/file.csv')

data.rename( columns={'Unnamed: 0':'id'}, inplace=True )

my_dict = {data['id'][i]: [data['tweets'][i], data['labels'][i]] for i in range(len(data['id']))}

mapping = {'bad': 0, 'neutral': 1, 'good': 2}
my_dict = {key: [value[0], mapping[value[1]]] for key, value in my_dict.items()}

print(my_dict)

# texts = []

stop_words = set(stopwords.words('english'))
def preprocess(value):
# remove the URLs from the tweet body
  url_regex = re.compile(r'https?://\S+')
  value = url_regex.sub('', value)

  # remove the english symbols and \n from the tweet body
  replaces = value.replace("/", "")
  replaces = replaces.replace("\\n", " ")
  value = re.sub('[\W]', ' ',replaces)

  # remove the stop words from the tweet body
  words = value.split()
  filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]
  value = ' '.join(filtered_words)
  return value

texts = []

for key in my_dict.keys():
  my_dict[key][0] = preprocess(my_dict[key][0])
  texts.append(my_dict[key][0])

print(my_dict)

lemmatizer = WordNetLemmatizer()

lemmated_tokens = []

for key in texts:
  lemmated_string = ""
  words = key.split()
  for word_2 in words:
    lemmated_string = lemmated_string + lemmatizer.lemmatize(word_2)+ " "
  lemmated_tokens.append(lemmated_string)
print(lemmated_tokens[0])

MAX_SEQUENCE_LENGTH = 1000
MAX_NUM_WORDS = 40000
def tokenize(tokens):
    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
    tokenizer.fit_on_texts(tokens)
    sequences = tokenizer.texts_to_sequences(tokens)
    word_index = tokenizer.word_index # the dictionary
    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
    return word_index,data

word_index ,data = tokenize(lemmated_tokens)
print(word_index)

VALIDATION_SPLIT = 0.2

y = [my_dict[key][1] for key in my_dict]

temp =[]
for i in y:
  if i == 0:
    temp_1=[1,0,0]
  elif i==1:
    temp_1 = [0,1,0]
  else:
      temp_1=[0,0,1]
  temp.append(temp_1)

X_train, X_test, y_train, y_test = train_test_split(data, temp, test_size=0.2, random_state=42)
X_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

X_train_array = np.array(X_train)
X_test_array = np.array(X_test)
y_train_array = np.array(y_train)
y_test_array = np.array(y_test)
x_val_array = np.array(x_val)
y_val_array = np.array(y_val)

def return_y(y):
    temp_Y =[]
    for i in range(len(y)):
        if y[i][0] == 1:
            temp_Y.append('bad')
        elif y[i][1] == 1:
            temp_Y.append('neutral')
        else:
            temp_Y.append('good')
    return temp_Y

def return_x(x):
    word_strings = []
    for word_array in x:
        temp_word =""
        for word in word_array:
            for key in word_index.keys():
                if word == word_index[key]:
                    temp_word = temp_word + key+ " "
        word_strings.append(temp_word)
    return word_strings

words_strings = return_x(X_train_array)
y_file_train =return_y(y_train_array)

filename = '/content/training_data.csv'
with open(filename, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['tweets', 'label'])  # Write header
    for i in range(len(words_strings)):
        writer.writerow([words_strings[i],y_file_train[i]])

words_strings = return_x(x_val_array)
y_file_train =return_y(y_val_array)

filename = '/content/validation_data.csv'

with open(filename, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['tweets', 'label'])  # Write header
    for i in range(len(words_strings)):
        writer.writerow([words_strings[i],y_file_train[i]])

words_strings = return_x(X_test_array)
y_file_train =return_y(y_test_array)

filename = '/content/testing_data.csv'

with open(filename, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['tweets', 'label'])  # Write header
    for i in range(len(words_strings)):
        writer.writerow([words_strings[i],y_file_train[i]])

def change_Y_to_Lists(y):
    temp =[]
    for i in y:
        if i == 'bad':
            temp_1=[1,0,0]
        elif i== 'neutral':
            temp_1 = [0,1,0]
        else:
            temp_1=[0,0,1]
        temp.append(temp_1)
    return temp

def read_data_file(temp_dic):
    X_List = []
    Y_List = []
    for i in range(len(temp_dic['tweets'])):
        X_List.append(temp_dic['tweets'][i])
        Y_List.append(temp_dic['label'][i])
    Y_List2 = change_Y_to_Lists(Y_List)
    return np.array(X_List) ,np.array(Y_List2)

training_data = pd.read_csv('training_data.csv', encoding='iso-8859-1')
training_data.head()

training_X_array , training_Y_array = read_data_file(training_data)
print(training_X_array)
print(training_Y_array)

validation_data = pd.read_csv('validation_data.csv', encoding='iso-8859-1')
validation_data.head()

validation_X_array , validation_Y_array = read_data_file(validation_data)
print(validation_X_array)
print(validation_Y_array)

testing_data = pd.read_csv('testing_data.csv', encoding='iso-8859-1')
testing_data.head()

testing_X_array , testing_Y_array = read_data_file(testing_data)
print(testing_X_array)
print(testing_Y_array)

testing_X_array,validation_X_array,training_X_array
total_x_List =[]
total_x_List.extend(training_X_array)
total_x_List.extend(validation_X_array)
total_x_List.extend(testing_X_array)

word_index_2,data2= tokenize(total_x_List) 

tarin_X_List =np.array(data2[0:len(training_X_array)])
num_x =len(training_X_array)+len(validation_X_array) 
valid_X_List =np.array(data2[len(training_X_array):num_x])
test_X_List = np.array(data2[num_x:len(testing_X_array)+num_x])

EMBEDDING_DIM = 300
embeddings_index = {}
with open('/content/drive/MyDrive/glove/Glove/glove.6B.300d.txt', 'r') as f:
  for line in f:
    values = line.split(sep=' ')
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
print('Found %s word vectors.' % len(embeddings_index))

embedding_matrix = np.zeros((len(word_index_2) + 1, EMBEDDING_DIM))#+1 to include the zerors vector for non-existing words
for word, i in word_index_2.items():
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    # words not found in embedding index will be all-zeros.
    embedding_matrix[i] = embedding_vector
print ('Shape of Embedding Matrix: ',embedding_matrix.shape)

embedding_layer = Embedding(len(word_index_2) + 1, #vocab size
    EMBEDDING_DIM, #embedding vector size
    weights=[embedding_matrix], #weights matrix
    input_length=MAX_SEQUENCE_LENGTH, #padded sequence length
    trainable=True)

def build_CNN(lr):
  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')
  embedded_sequences = embedding_layer(sequence_input)

  filter_sizes = [3, 3, 4]

  conv_layers = []
  for filter_size in filter_sizes:
      conv_layer = Conv1D(128, kernel_size=filter_size, activation='relu')(embedded_sequences)
      pool_layer = GlobalMaxPooling1D()(conv_layer)
      conv_layers.append(pool_layer)

  merged_layer = Concatenate()(conv_layers)
  flatten_layer = Flatten()(merged_layer)
  dropout_layer = Dropout(0.5)(flatten_layer)

  dense_layer1 = Dense(128, activation='relu')(dropout_layer)
  output_layer = Dense(3, activation='softmax')(dense_layer1)

  cnn_model = Model(inputs=sequence_input, outputs=output_layer)
  cnn_model = Model(inputs=sequence_input, outputs=output_layer)
  cnn_model = Model(sequence_input, output_layer)
  cnn_model.compile(loss='categorical_crossentropy',optimizer=RMSprop(lr=lr),metrics=['acc'])
  cnn_model.summary()
  return cnn_model

lr_list = [0.0001, 0.001, 0.01] # List of learning rates to try
n_epochs = 10 # Number of epochs to train each model
histories = [] # List to store the histories of each model
for lr in lr_list:
    CNN_model = build_CNN(lr=lr)
    history = CNN_model.fit(X_train_array, y_train_array, epochs=n_epochs, validation_data=(x_val_array, y_val_array), callbacks=[LearningRateScheduler(lambda epoch: lr)], batch_size=128)
    CNN_model.evaluate(X_test_array, y_test_array)
    histories.append(history)

# Plot the validation loss for each model
plt.figure()
for i, history in enumerate(histories):
    plt.plot(history.history['val_loss'], label='LR={}'.format(lr_list[i]))
plt.xlabel('Epoch')
plt.ylabel('Validation loss')
plt.legend()
plt.show()

# model.predict(X_test_array)

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
def build_LSTM(lr):
  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')
  embedded_sequences = embedding_layer(sequence_input)

  # Define the LSTM layer
  lstm_layer = LSTM(128)(embedded_sequences)
  # Add a dropout layer to reduce overfitting
  dropout_layer = Dropout(0.5)(lstm_layer)
  # Add a dense output layer with softmax activation for multiclass classification
  output_layer = Dense(3, activation='softmax')(dropout_layer)
  # Define the model with the input and output layers
  LSTM_model = Model(inputs=sequence_input, outputs=output_layer)
  LSTM_model.compile(loss='categorical_crossentropy',optimizer=RMSprop(lr=lr),metrics=['acc'])
  LSTM_model.summary()
  return LSTM_model

lr_list_LSTM = [0.0001, 0.001, 0.01] # List of learning rates to try
n_epochs = 10 # Number of epochs to train each model
LSTM_histories = [] # List to store the histories of each model
for lr in lr_list_LSTM:
    LSTM_model = build_LSTM(lr=lr)
    LSTM_history = LSTM_model.fit(X_train_array, y_train_array, epochs=n_epochs, validation_data=(x_val_array, y_val_array),
                                  callbacks=[LearningRateScheduler(lambda epoch: lr)], batch_size=128)
    LSTM_model.evaluate(X_test_array, y_test_array)
    LSTM_histories.append(LSTM_history)

# Plot the validation loss for each model
plt.figure()
for i, LSTM_history in enumerate(LSTM_histories):
    plt.plot(LSTM_history.history['val_loss'], label='LR={}'.format(lr_list_LSTM[i]))
plt.xlabel('Epoch')
plt.ylabel('Validation loss')
plt.legend()
plt.show()

# print('Acuracy on testing set:')
# LSTM_model.evaluate(X_test_array, y_test_array)

# LSTM_model.predict(X_test_array)

tweet = input("Enter your tweet: ")
print(tweet)

tweet = preprocess(tweet)

lemmatize_token = []
words = tweet.split()
lemmatize_string = ""
for word_2 in words:
  lemmatize_string = lemmatize_string + lemmatizer.lemmatize(word_2)+" "
tweet = lemmatize_string
lemmatize_token.append(tweet)

tweet_word_index, tweet_data=tokenize(lemmatize_token)

label_vec = CNN_model.predict(tweet_data[0].reshape(1,-1))

actual_labels = {'negative': 0, 'neutral': 1, 'positive': 2}
probs = np.array(label_vec)
index = np.argmax(probs)
output_label = list(actual_labels.keys())[list(actual_labels.values()).index(index)]

print(output_label)